import pickle
import os
import tiktoken
from pathlib import Path
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass
import pandas as pd
from raptor.tree_structures import Tree, Node

@dataclass
class TokenCostInfo:
    """í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤"""
    embedding_tokens: int = 0
    summary_input_tokens: int = 0
    summary_output_tokens: int = 0
    total_tokens: int = 0
    embedding_cost: float = 0.0
    summary_cost: float = 0.0
    total_cost: float = 0.0

class TokenCostAnalyzer:
    """pkl íŒŒì¼ì—ì„œ íŠ¸ë¦¬ë¥¼ ë¡œë“œí•˜ê³  í† í°ëŸ‰ ë° ë¹„ìš©ì„ ë¶„ì„í•˜ëŠ” í´ë˜ìŠ¤"""
    
    # Azure OpenAI ê°€ê²© (USD per 1K tokens)
    PRICING = {
        "text-embedding-3-large": 0.00013,  # per 1K tokens
        "gpt-4o": {
            "input": 0.0025,   # per 1K input tokens
            "output": 0.01     # per 1K output tokens
        }
    }
    
    def __init__(self):
        # GPT-4o í† í¬ë‚˜ì´ì € ì‚¬ìš©
        self.tokenizer = tiktoken.encoding_for_model("gpt-4o")
    
    def count_tokens(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°"""
        if not text or not isinstance(text, str):
            return 0
        return len(self.tokenizer.encode(text))
    
    def load_tree_from_pkl(self, pkl_path: str) -> Tree:
        """pkl íŒŒì¼ì—ì„œ íŠ¸ë¦¬ë¥¼ ë¡œë“œ"""
        try:
            with open(pkl_path, 'rb') as f:
                tree = pickle.load(f)
            if not isinstance(tree, Tree):
                raise ValueError(f"Loaded object is not a Tree instance: {type(tree)}")
            return tree
        except Exception as e:
            raise ValueError(f"Failed to load tree from {pkl_path}: {e}")
    
    def analyze_tree_tokens(self, tree: Tree) -> TokenCostInfo:
        """íŠ¸ë¦¬ì˜ ëª¨ë“  ë…¸ë“œë¥¼ ë¶„ì„í•˜ì—¬ í† í°ëŸ‰ê³¼ ë¹„ìš©ì„ ê³„ì‚°"""
        cost_info = TokenCostInfo()
        
        # ëª¨ë“  ë…¸ë“œ ë¶„ì„
        for node_id, node in tree.all_nodes.items():
            if not isinstance(node, Node):
                continue
                
            # ì„ë² ë”© í† í° ê³„ì‚° (ê° ë…¸ë“œì˜ í…ìŠ¤íŠ¸)
            embedding_tokens = self.count_tokens(node.text)
            cost_info.embedding_tokens += embedding_tokens
            
            # ìš”ì•½ í† í° ê³„ì‚° (leaf ë…¸ë“œê°€ ì•„ë‹Œ ê²½ìš°, ì¦‰ ìš”ì•½ëœ ë…¸ë“œì¸ ê²½ìš°)
            if node.children:  # ìì‹ì´ ìˆë‹¤ë©´ ìš”ì•½ëœ ë…¸ë“œ
                # ìì‹ ë…¸ë“œë“¤ì˜ í…ìŠ¤íŠ¸ë¥¼ í•©ì³ì„œ ì…ë ¥ í† í° ê³„ì‚°
                child_texts = []
                for child_id in node.children:
                    if child_id in tree.all_nodes:
                        child_node = tree.all_nodes[child_id]
                        child_texts.append(child_node.text)
                
                if child_texts:
                    # ì…ë ¥: ìì‹ ë…¸ë“œë“¤ì˜ í…ìŠ¤íŠ¸ + ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
                    input_text = "\n".join(child_texts)
                    system_prompt = "ë‹¹ì‹ ì€ ë³´í—˜ì•½ê´€ì— ëŒ€í•´ ì•ˆì „í•œ ìš”ì•½ì„ ì œê³µí•˜ëŠ” ìœ ìš©í•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤..."
                    user_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”:\n\n{input_text}"
                    
                    input_tokens = (
                        self.count_tokens(system_prompt) + 
                        self.count_tokens(user_prompt)
                    )
                    
                    # ì¶œë ¥: í˜„ì¬ ë…¸ë“œì˜ í…ìŠ¤íŠ¸ (ìš”ì•½ ê²°ê³¼)
                    output_tokens = self.count_tokens(node.text)
                    
                    cost_info.summary_input_tokens += input_tokens
                    cost_info.summary_output_tokens += output_tokens
        
        # ì´ í† í° ê³„ì‚°
        cost_info.total_tokens = (
            cost_info.embedding_tokens + 
            cost_info.summary_input_tokens + 
            cost_info.summary_output_tokens
        )
        
        # ë¹„ìš© ê³„ì‚°
        cost_info.embedding_cost = (cost_info.embedding_tokens / 1000) * self.PRICING["text-embedding-3-large"]
        cost_info.summary_cost = (
            (cost_info.summary_input_tokens / 1000) * self.PRICING["gpt-4o"]["input"] +
            (cost_info.summary_output_tokens / 1000) * self.PRICING["gpt-4o"]["output"]
        )
        cost_info.total_cost = cost_info.embedding_cost + cost_info.summary_cost
        
        return cost_info
    
    def analyze_pkl_file(self, pkl_path: str) -> Tuple[str, TokenCostInfo, Dict[str, Any]]:
        """ë‹¨ì¼ pkl íŒŒì¼ì„ ë¶„ì„"""
        filename = Path(pkl_path).stem
        
        try:
            tree = self.load_tree_from_pkl(pkl_path)
            cost_info = self.analyze_tree_tokens(tree)
            
            # íŠ¸ë¦¬ êµ¬ì¡° ì •ë³´
            tree_info = {
                "total_nodes": len(tree.all_nodes),
                "num_layers": tree.num_layers,
                "root_nodes": len(tree.root_nodes),
                "leaf_nodes": len(tree.leaf_nodes),
                "layer_distribution": {layer: len(nodes) for layer, nodes in tree.layer_to_nodes.items()}
            }
            
            return filename, cost_info, tree_info
            
        except Exception as e:
            print(f"Error analyzing {pkl_path}: {e}")
            return filename, TokenCostInfo(), {}
    
    def analyze_all_pkl_files(self, pkl_directory: str = "results") -> pd.DataFrame:
        """ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  pkl íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜"""
        pkl_dir = Path(pkl_directory)
        pkl_files = list(pkl_dir.glob("*.pkl"))
        
        if not pkl_files:
            print(f"No pkl files found in {pkl_directory}")
            return pd.DataFrame()
        
        results = []
        
        print(f"Analyzing {len(pkl_files)} pkl files...")
        
        for i, pkl_file in enumerate(pkl_files, 1):
            print(f"[{i}/{len(pkl_files)}] Analyzing: {pkl_file.name}")
            
            filename, cost_info, tree_info = self.analyze_pkl_file(str(pkl_file))
            
            result = {
                "filename": filename,
                "embedding_tokens": cost_info.embedding_tokens,
                "summary_input_tokens": cost_info.summary_input_tokens,
                "summary_output_tokens": cost_info.summary_output_tokens,
                "total_tokens": cost_info.total_tokens,
                "embedding_cost_usd": round(cost_info.embedding_cost, 6),
                "summary_cost_usd": round(cost_info.summary_cost, 6),
                "total_cost_usd": round(cost_info.total_cost, 6),
                "total_nodes": tree_info.get("total_nodes", 0),
                "num_layers": tree_info.get("num_layers", 0),
                "root_nodes": tree_info.get("root_nodes", 0),
                "leaf_nodes": tree_info.get("leaf_nodes", 0),
            }
            
            results.append(result)
        
        df = pd.DataFrame(results)
        return df
    
    def print_summary_statistics(self, df: pd.DataFrame):
        """ë¶„ì„ ê²°ê³¼ì˜ ìš”ì•½ í†µê³„ë¥¼ ì¶œë ¥"""
        if df.empty:
            print("No data to summarize")
            return
        
        print("\n" + "="*80)
        print("ğŸ“Š TOKEN & COST ANALYSIS SUMMARY")
        print("="*80)
        
        # ì „ì²´ í†µê³„
        total_files = len(df)
        total_embedding_tokens = df['embedding_tokens'].sum()
        total_summary_input_tokens = df['summary_input_tokens'].sum()
        total_summary_output_tokens = df['summary_output_tokens'].sum()
        total_tokens = df['total_tokens'].sum()
        total_cost = df['total_cost_usd'].sum()
        
        print(f"ğŸ“ Total Files Analyzed: {total_files}")
        print(f"ğŸ”¢ Total Tokens: {total_tokens:,}")
        print(f"   - Embedding Tokens: {total_embedding_tokens:,}")
        print(f"   - Summary Input Tokens: {total_summary_input_tokens:,}")
        print(f"   - Summary Output Tokens: {total_summary_output_tokens:,}")
        print(f"ğŸ’° Total Cost: ${total_cost:.6f} USD")
        print(f"   - Embedding Cost: ${df['embedding_cost_usd'].sum():.6f} USD")
        print(f"   - Summary Cost: ${df['summary_cost_usd'].sum():.6f} USD")
        
        # í‰ê·  í†µê³„
        print(f"\nğŸ“ˆ Average per File:")
        print(f"   - Tokens: {df['total_tokens'].mean():.0f}")
        print(f"   - Cost: ${df['total_cost_usd'].mean():.6f} USD")
        print(f"   - Nodes: {df['total_nodes'].mean():.1f}")
        print(f"   - Layers: {df['num_layers'].mean():.1f}")
        
        # ìµœëŒ€/ìµœì†Œ
        max_cost_file = df.loc[df['total_cost_usd'].idxmax()]
        min_cost_file = df.loc[df['total_cost_usd'].idxmin()]
        
        print(f"\nğŸ” Highest Cost File:")
        print(f"   - {max_cost_file['filename']}: ${max_cost_file['total_cost_usd']:.6f} USD ({max_cost_file['total_tokens']:,} tokens)")
        
        print(f"ğŸ”» Lowest Cost File:")
        print(f"   - {min_cost_file['filename']}: ${min_cost_file['total_cost_usd']:.6f} USD ({min_cost_file['total_tokens']:,} tokens)")
        
        print("="*80)

    def analyze_single_file(self, pkl_path: str, detailed: bool = True):
        """ë‹¨ì¼ pkl íŒŒì¼ì„ ë¶„ì„í•˜ê³  ìƒì„¸ ì •ë³´ë¥¼ ì¶œë ¥"""
        print(f"ğŸ” Analyzing: {pkl_path}")
        print("-" * 60)
        
        filename, cost_info, tree_info = self.analyze_pkl_file(pkl_path)
        
        if cost_info.total_tokens == 0:
            print("âŒ Failed to analyze file or no tokens found")
            return
        
        # ê¸°ë³¸ ì •ë³´ ì¶œë ¥
        print(f"ğŸ“„ File: {filename}")
        print(f"ğŸŒ³ Tree Structure:")
        print(f"   - Total Nodes: {tree_info.get('total_nodes', 0)}")
        print(f"   - Layers: {tree_info.get('num_layers', 0)}")
        print(f"   - Root Nodes: {tree_info.get('root_nodes', 0)}")
        print(f"   - Leaf Nodes: {tree_info.get('leaf_nodes', 0)}")
        
        print(f"\nğŸ”¢ Token Usage:")
        print(f"   - Embedding Tokens: {cost_info.embedding_tokens:,}")
        print(f"   - Summary Input Tokens: {cost_info.summary_input_tokens:,}")
        print(f"   - Summary Output Tokens: {cost_info.summary_output_tokens:,}")
        print(f"   - Total Tokens: {cost_info.total_tokens:,}")
        
        print(f"\nğŸ’° Cost Breakdown (USD):")
        print(f"   - Embedding Cost: ${cost_info.embedding_cost:.6f}")
        print(f"   - Summary Cost: ${cost_info.summary_cost:.6f}")
        print(f"   - Total Cost: ${cost_info.total_cost:.6f}")
        
        if detailed and tree_info.get('layer_distribution'):
            print(f"\nğŸ“Š Layer Distribution:")
            for layer, count in tree_info['layer_distribution'].items():
                print(f"   - Layer {layer}: {count} nodes")
        
        print("-" * 60)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Analyze token usage and costs from RAPTOR tree pkl files")
    parser.add_argument("--file", "-f", type=str, help="Analyze a single pkl file")
    parser.add_argument("--directory", "-d", type=str, default="results", 
                       help="Directory containing pkl files (default: results)")
    parser.add_argument("--output", "-o", type=str, default="token_cost_analysis.csv",
                       help="Output CSV file name (default: token_cost_analysis.csv)")
    parser.add_argument("--detailed", action="store_true", 
                       help="Show detailed information for single file analysis")
    
    args = parser.parse_args()
    
    analyzer = TokenCostAnalyzer()
    
    if args.file:
        # ë‹¨ì¼ íŒŒì¼ ë¶„ì„
        analyzer.analyze_single_file(args.file, args.detailed)
    else:
        # ëª¨ë“  pkl íŒŒì¼ ë¶„ì„
        df = analyzer.analyze_all_pkl_files(args.directory)
        
        if df.empty:
            print(f"No pkl files found in {args.directory}")
            return
        
        # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
        df.to_csv(args.output, index=False)
        print(f"\nğŸ’¾ Results saved to: {args.output}")
        
        # ìš”ì•½ í†µê³„ ì¶œë ¥
        analyzer.print_summary_statistics(df)
        
        # ìƒìœ„ 10ê°œ íŒŒì¼ ì¶œë ¥
        print(f"\nğŸ† Top 10 Most Expensive Files:")
        top_10 = df.nlargest(10, 'total_cost_usd')[['filename', 'total_tokens', 'total_cost_usd']]
        for i, row in top_10.iterrows():
            print(f"   {row['filename']}: {row['total_tokens']:,} tokens, ${row['total_cost_usd']:.6f} USD")

if __name__ == "__main__":
    main()